{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_wav = \"Recording_0724\"  \n",
    "infer_wav = target_wav[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.ndimage import shift\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.signal import medfilt\n",
    "import datetime\n",
    "from pyannote.database.util import load_rttm\n",
    "import speech_recognition as sr\n",
    "\n",
    "from eend.pytorch_backend.models import TransformerModel, TransformerCNNModel\n",
    "from eend import feature\n",
    "from eend import kaldi_data\n",
    "import sys\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): TransformerModel(\n",
       "    (encoder): Linear(in_features=345, out_features=256, bias=True)\n",
       "    (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "def _gen_chunk_indices(data_len, chunk_size):\n",
    "    step = chunk_size\n",
    "    start = 0\n",
    "    while start < data_len:\n",
    "        end = min(data_len, start + chunk_size)\n",
    "        yield start, end\n",
    "        start += step\n",
    "\n",
    "\n",
    "in_size = feature.get_input_dim(\n",
    "            200,\n",
    "            7,\n",
    "            \"logmel23_mn\")\n",
    "\n",
    "## TransformerCNNModel or TransformerModel\n",
    "model = TransformerModel(\n",
    "            n_speakers=2,\n",
    "            in_size=in_size,\n",
    "            n_units=256,\n",
    "            n_heads=4,\n",
    "            n_layers=4,\n",
    "            has_pos=False\n",
    "            )\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "if device.type == \"cuda\":\n",
    "    model = nn.DataParallel(model, list(range(1)))\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./model/transformer334.th\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='./demo_file/Recording_.wav'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "ori_path = \"./demo_file/\" + target_wav + \".wav\"   \n",
    "infer_path = \"./demo_file/\" + infer_wav + \".wav\"\n",
    "sound = AudioSegment.from_wav(ori_path)\n",
    "sound = sound.set_frame_rate(8000)  # sample rate \n",
    "sound = sound.set_channels(1) # 改成單聲道\n",
    "sound.export(infer_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Infer\n",
    "audio_data, rate = sf.read(infer_path)\n",
    "Y = feature.stft(\"True\", audio_data, 200, 80)\n",
    "Y = feature.transform(Y, transform_type=\"logmel23_mn\")\n",
    "Y = feature.splice(Y, 7)\n",
    "Y = Y[::10]\n",
    "out_chunks = []\n",
    "with torch.no_grad():\n",
    "    hs = None\n",
    "    for start, end in _gen_chunk_indices(len(Y), 4000):\n",
    "        Y_chunked = torch.from_numpy(Y[start:end])\n",
    "        Y_chunked.to(device)               \n",
    "        ys = model([Y_chunked], activation=torch.sigmoid)\n",
    "        out_chunks.append(ys[0].cpu().detach().numpy())\n",
    "            \n",
    "outfname = infer_path[:-4] + '.h5'\n",
    "outdata = np.vstack(out_chunks)\n",
    "with h5py.File(outfname, 'w') as wf:\n",
    "    wf.create_dataset('T_hat', data=outdata)\n",
    "    \n",
    "##create RTTM file    \n",
    "threshold = 0.5\n",
    "median = 11\n",
    "frame_shift = 80\n",
    "subsampling = 10\n",
    "sampling_rate = 8000\n",
    "\n",
    "out_rttm_file = infer_path[:-4] + \".rttm\"\n",
    "with open(out_rttm_file, 'w') as wf:\n",
    "    session, _ = os.path.splitext(os.path.basename(outfname))\n",
    "    data = h5py.File(outfname, 'r')\n",
    "    a = np.where(data['T_hat'][:] > threshold, 1, 0)\n",
    "    if median > 1:\n",
    "        a = medfilt(a, (median, 1))\n",
    "    for spkid, frames in enumerate(a.T):\n",
    "        frames = np.pad(frames, (1, 1), 'constant')\n",
    "        changes, = np.where(np.diff(frames, axis=0) != 0)\n",
    "        fmt = \"SPEAKER {:s} 1 {:7.2f} {:7.2f} <NA> <NA> {:s} <NA>\"\n",
    "        for s, e in zip(changes[\n",
    "            ::2], changes[1::2]):\n",
    "            print(fmt.format(\n",
    "                      session,\n",
    "                      s * frame_shift * subsampling / sampling_rate,\n",
    "                      (e - s) * frame_shift * subsampling / sampling_rate,\n",
    "                      session + \"_\" + str(spkid)), file=wf)\n",
    "\n",
    "##read RTTM\n",
    "with open(out_rttm_file, 'r', encoding='utf-8') as f:\n",
    "    info = []\n",
    "    for line in f.readlines():\n",
    "        l = line.strip()\n",
    "        l = l.split(\" \")\n",
    "        a = []\n",
    "        for i in range(len(l)):\n",
    "            if(l[i] != '' and l[i] != '<NA>'): \n",
    "                a.append(l[i])    \n",
    "        info.append(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a5f9ac210743f49a6ac3224cc0cc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 老師要得逐字稿\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "r = sr.Recognizer()\n",
    "WAV = sr.AudioFile(ori_path)\n",
    "transcript = []\n",
    "for i in tqdm(range(len(info))):\n",
    "    offset = float(info[i][3])\n",
    "    duration = float(info[i][4])\n",
    "    with WAV as source:\n",
    "        if(duration<0.3): continue\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio = r.record(source, offset=offset-1.0, duration=duration+0.5)\n",
    "        text = r.recognize_google(audio, language='zh-CN',show_all = True)\n",
    "        if text != [] : \n",
    "            text = text['alternative'][0]['transcript']\n",
    "#            print('speaker', info[i][5], '(', str(datetime.timedelta(seconds=round(offset))), 's -->',\n",
    "#                                              str(datetime.timedelta(seconds=round(offset+duration))), 's) : ', text)\n",
    "            temp = [info[i][5],offset,duration,text]\n",
    "    transcript.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker 0 :  0:01  --> 0:03 請問您的編號是多少\n",
      "speaker 1 :  0:04  --> 0:05 和54088\n",
      "speaker 0 :  0:05  --> 0:08 好的請問您這兩週去了哪呢\n",
      "speaker 1 :  0:08  --> 0:12 我去過學校公司全聯茶店還有7-ELEVEN\n",
      "speaker 0 :  0:12  --> 0:13 好的謝謝\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##print transcript\n",
    "transcript.sort(key = lambda s: s[1])\n",
    "for i in range(len(transcript)):\n",
    "    print('speaker', transcript[i][0][-1],': ', str(datetime.timedelta(seconds=round(transcript[i][1])))[-4:], ' -->',\n",
    "                                          str(datetime.timedelta(seconds=round(transcript[i][2]+transcript[i][1])))[-4:],\n",
    "                                          transcript[i][3])\n",
    "    \n",
    "##plot diarization figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "\n",
    "for i in range(len(info)):\n",
    "    offset = float(info[i][3])\n",
    "    duration = float(info[i][4])\n",
    "    if info[i][5][-1]=='0':\n",
    "        plt.barh(0, height=1, width=duration, left=offset, color=\"blue\")\n",
    "    elif info[i][5][-1]=='1':\n",
    "        plt.barh(1, height=1, width=duration, left=offset, color=\"red\")\n",
    "\n",
    "n=2\n",
    "plt.yticks(np.arange(n), ['Interviewer', 'Interviewee'])\n",
    "plt.xticks(np.arange(0, (len(audio_data)/rate)+5, step=5))\n",
    "plt.xlabel(\"Seconds\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
